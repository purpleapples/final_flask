# -*- coding: utf-8 -*-
"""[컴퓨터] 3. 분류 Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1trbWDgtDtvS_k2z0qvk9M7hMXfww-9ag

# 라이브러리 및 DB 연결
"""

# colab google 드라이브 마운트
from google.colab import drive
drive.mount("/content/drive")

from tqdm.notebook import tqdm
from collections import Counter
import pandas as pd
import re
import numpy as np
import pickle
import matplotlib.pyplot as plt

# 데이터 불러오기
with open(SAVE_PATH + "compParts_tokenized_mix.pickle", "rb") as file:
    data_origin = pickle.load(file)

"""# 데이터 불러오기
  - pandas 처리
  - 2-1과 데이터가 다름

# Labeling
"""

data = data_origin.copy()

for index, category in enumerate(data.category3.value_counts().keys()):
    data.loc[data["category3"] == category, 'label'] = index

# label int로 변환
data = data.astype({'label': int})
data

print(data.category3.value_counts())
print(len(data.category3.value_counts()))

"""# __임윤택 : 데이터 정제"""

# 데이터량을 맞추기 위해 일부 추출
temp1 = data[data["label"]==0].sample(frac=0.3)
temp2 = data[data["label"]==1].sample(frac=0.3)

drop_index1 = data[data["label"]==0].index
drop_index2 = data[data["label"]==1].index

data.drop(drop_index1, inplace=True)
data.drop(drop_index2, inplace=True)

data = pd.concat([data, temp1, temp2])

print(data.category3.value_counts())
print(len(data.category3.value_counts()))









"""# Train_Test_Split"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data.token_morphs, data.label, test_size=0.2, stratify=data.label)

x_train

x_test

print('최대 길이 : {}'.format(max(len(l) for l in x_train)))
print('평균 길이 : {}'.format(sum(map(len, x_train)) / len(x_train)))
print(len(x_train))

print('최대 길이 : {}'.format(max(len(l) for l in x_test)))
print('평균 길이 : {}'.format(sum(map(len, x_test)) / len(x_test)))
print(len(x_test))

plt.hist([len(s) for s in x_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('length of samples')
plt.show()

x_train = x_train[x_train.apply(lambda x: len(str(x)) < 2000)]
x_test = x_test[x_test.apply(lambda x: len(str(x)) < 2000)]
y_train = y_train[x_train.index]
y_test = y_test[x_test.index]
x_train

print('최대 길이 : {}'.format(max(len(l) for l in x_train)))
print('평균 길이 : {}'.format(sum(map(len, x_train)) / len(x_train)))
print(len(x_train))

print('최대 길이 : {}'.format(max(len(l) for l in x_test)))
print('평균 길이 : {}'.format(sum(map(len, x_test)) / len(x_test)))
print(len(x_test))

plt.hist([len(s) for s in x_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('length of samples')
plt.show()

"""# Mecab - 자체 토크나이저를 사용하므로 형태소 분류기 영역 삭제

# Tokenizer
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)

threshold = 3 # 임계점
total_cnt = len(tokenizer.word_index) # 단어 개수

rare_cnt = 0 # 등장 빈도가 threshold보다 적은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총합

rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총합

for key, value in tokenizer.word_counts.items():
  total_freq = total_freq + value
  
  if value < threshold:
    rare_cnt += 1 # 희소 단어의 개수
    rare_freq = rare_freq + value # 희소 단어의 등장 빈도의 총합

print('단어 집합의 크기 : ', total_cnt)
print('등장 빈도가 2번 이하인 희귀 단어의 수 : ', rare_cnt)
print('단어 집합에서 회귀 단어의 비율 : ', (rare_cnt / total_cnt) * 100)
print('전체 등장 빈도에서 희귀 단어 등장 빈도 비율 : ', (rare_freq / total_freq) * 100)

"""# 정수 인코딩"""

X_train = tokenizer.texts_to_sequences(x_train)
X_test = tokenizer.texts_to_sequences(x_test)

print(X_train[:3])

print(X_test[:3])

from keras.utils import np_utils
y_tr = np_utils.to_categorical(y_train)
y_te = np_utils.to_categorical(y_test)

"""# 패딩 설정"""

print('최대 길이 : ', max(len(l) for l in X_train))
print('평균 길이 : ', sum(map(len, X_train)) / len(X_train))

plt.hist([ len(s) for s in X_train ], bins=50)
plt.xlabel('Length of Samples')
plt.ylabel('Number of Samples')
plt.show()

def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if len(s) <= max_len:
      cnt = cnt + 1
  print('전체 샘플 중 길이가 {} 이하인 샘플의 비율 : {}'.format(max_len, (cnt / len(nested_list) *100)))

max_len = 370
below_threshold_len(max_len, X_train)
below_threshold_len(500, X_train)
below_threshold_len(800, X_train)

# 패딩 적용( max_len = 370 )
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

"""# LSTM 분류"""

from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

vocab_size = 104079

model = Sequential()
model.add(Embedding(vocab_size, max_len))
model.add(Bidirectional(LSTM(128, return_sequences=True)))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(9, activation='softmax'))

# es = EarlyStopping(monitor  ='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('/content/drive/MyDrive/Final Project/New_Project/model/LSTM/best_LSTM_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['acc']               
)
model.summary()

X_train.shape

y_tr.shape

history = model.fit(X_train, y_tr,
                    epochs=10,
                    callbacks=mc,
                    batch_size = 64,
                    validation_split = 0.25)

from keras_preprocessing.text import tokenizer_from_json
import json

import io
tokenizer_json = tokenizer.to_json()
with io.open("/content/drive/MyDrive/Final Project/New_Project/model/LSTM/tokenizer.json", 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

"""# Predict - LSTM"""

x_test

y_test

from tensorflow.keras.models import load_model
load_model = load_model('/content/best_LSTM_model.h5')

prediction = load_model.predict(X_test)

predictions = []
for i in tqdm(range(0, len(X_test))):
    predictions.append(np.argmax(prediction[i]))

y_test = pd.DataFrame(y_test)
y_test[0:1]

prediction = model.predict(X_test)

np.argmax(prediction[1])

y_test1 = y_test.reindex().label
y_test1

"""# Load_Model
모델을 로드한 뒤 아래에 있는 과정을 거친 X_test를 모델에 predict 해야 한다.

"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)
X_test = tokenizer.texts_to_sequences(x_test)

load_model = load_model('/content/best_LSTM_model.h5')
prediction = load_model.predict(X_test)
predictions = []
for i in tqdm(range(0, len(X_test))):
  predictions.append(np.argmax(prediction[i]))
table = pd.concat([data.loc[x_test.index].content.reset_index(), y_test1.reset_index().label, pd.Series(predictions)], axis=1)
table.columns = ['index', 'content', 'label', 'prediction']
table['difference'] = table['label']!=table['prediction']

table

data.category3.value_counts()